# 2026-02-06

## Sessione notturna/mattutina (02:42 - 12:30)

### Ecosistema Go - Build fixes
- **framegotui**: import non usato + ctx fix + quic-go v0.59.0 (CVE fix)
- **saigobro**: creato cmd/saigobro/main.go (repo era vuoto)
- **memogo**: imports aimeitempi â†’ database-storm + deps aggiornati (7 CVE fix)
- **saidisigo**: go.sum aggiunto per tui module
- **neoncortex**: TS null check fix + npm install con --legacy-peer-deps

### Smembramento NeonCortex
NeonCortex (Angular/TS) smembrato nell'ecosistema Go:
- **saidisigo**: +`go/saidisigo/actions.go` - tipi Action, Priority, Status, ParsedResponse
- **ideaeater-go**: +`cmd/main.go`, +`internal/analyzer/` - pipeline processing
- Aggiunti come submodules in autoschei

### FrameGoTUI v1.1 - Refactoring completo
- **internal/** struttura: core/, config/, grpc/, tui/
- **ConnectRPC** integration con proto generation
- **Audit completo**: go vet, golangci-lint, tutti i test passano
- Fix: errcheck, gosec (permessi 0o600), unused fields, exhaustive switch

### Voxtral Realtime - Ricerca per saidisigo
Mistral ha rilasciato Voxtral Realtime - STT sub-200ms, Apache 2.0
- Documentato in `saidisigo/research/voxtral-realtime.md`
- **IMPORTANTE**: Richiede vLLM, NON transformers standard
- Script: `voxtral_api_test.py` (API $0.006/min), `voxtral_vllm_modal.py` (self-hosted)

### Opzioni STT gratuite testate
- **Groq Whisper**: gratis, veloce, script `groq_whisper.sh`
- **HuggingFace Spaces**: instabili oggi
- **GPU Cloud**: Modal.com raccomandato ($30 free)

### GPU Cloud Options
Documentato in `saidisigo/research/GPU_CLOUD_OPTIONS.md`:
- Modal.com: $30/mese free + $25k startup program
- GCP: $300 nuovi utenti
- RunPod: economico ma no free tier fisso

### TTS per Desy
Generato messaggio audio (3:54) spiegando il lavoro di Sergio a Desy.
File: `/tmp/messaggio_desy_completo.mp3`

---

## Sessione pomeridiana (17:00 - 18:00)

### ðŸŽ¤ Saidisigo Realtime Engine - MAJOR FEATURE

Convergenza dei repo conversazionali (voice-cli-engine + saidisigo) in un unico motore realtime:

**Nuovi moduli creati:**
- `realtime/engine.go` - Full-duplex orchestrator con barge-in
- `realtime/session.go` - Multi-session management  
- `realtime/audio.go` - Audio I/O + echo suppression
- `internal/vad/vad.go` - Voice Activity Detection
- `internal/stt/voxtral.go` - Voxtral Realtime provider (sub-200ms)
- `internal/stt/groq.go` - Groq Whisper provider (free tier)
- `cmd/realtime/` - Demo CLI

**Caratteristiche chiave:**
- **Barge-in support** - interrompi l'AI senza troncare le frasi
- **Full-duplex** - parla e ascolta contemporaneamente
- **VAD** - rileva automaticamente quando parli
- **Context preservation** - ricorda cosa stava dicendo prima dell'interruzione
- **Streaming word-by-word** - risposta progressiva naturale

**Latenza target:**
- STT: < 200ms (Voxtral Realtime)
- LLM: < 500ms
- TTS: < 75ms (ElevenLabs Flash)
- **Round-trip: < 800ms**

**Build & test OK** - demo funzionante che simula conversazione con streaming

---

### Lesson learned
- Voxtral Realtime ha architettura custom â†’ solo vLLM, no transformers
- HuggingFace Spaces possono essere instabili
- Per testing rapido: Groq o Mistral API > self-hosted
- Go multi-module setup complicato â†’ meglio single module con replace
- Barge-in richiede: VAD + cancellazione TTS + context preservation
